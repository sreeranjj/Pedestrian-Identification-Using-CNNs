{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"./processed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image labels from annotations\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "with open(data_path + \"category.json\", \"r\") as f:\n",
    "    categories = json.load(f)\n",
    "with open(data_path + \"metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "    \n",
    "category_indices = {cat['name']: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "image_labels = {}\n",
    "\n",
    "for image, values in metadata.items():\n",
    "    image_file_name = values[\"filename\"]\n",
    "    \n",
    "    if image_file_name not in image_labels:\n",
    "        image_labels[image_file_name] = [0] * len(categories)\n",
    "        \n",
    "    for cat in values[\"categories\"]:\n",
    "        cat_index = category_indices[cat]\n",
    "        image_labels[image_file_name][cat_index] = 1\n",
    "        \n",
    "image_labels = list(image_labels.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Torch Dataset object\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, image_labels, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_labels (list of tuples): List of tuples (image_path, label_vector).\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_labels = image_labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_labels[idx][0])\n",
    "        image = Image.open(img_name)\n",
    "        labels = self.image_labels[idx][1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for project update using first 10000 images\n",
    "image_labels = image_labels[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_data, val_data = train_test_split(image_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MultiLabelDataset(image_labels=train_data, root_dir=data_path+\"images/\", transform=transform)\n",
    "val_dataset = MultiLabelDataset(image_labels=val_data, root_dir=data_path+\"images/\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hrida\\miniforge3\\envs\\new_environment\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hrida\\miniforge3\\envs\\new_environment\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\hrida/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:34<00:00, 16.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "# define our model\n",
    "\n",
    "model_vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "for param in model_vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_features = model_vgg16.classifier[0].in_features\n",
    "num_categories = len(categories)\n",
    "\n",
    "model_vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(4096, num_categories),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_vgg16 = model_vgg16.to(device)\n",
    "\n",
    "optimizer = Adam(model_vgg16.classifier.parameters(), lr=0.001)\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [03:23<00:00,  1.23it/s]\n",
      "100%|██████████| 63/63 [00:48<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.20136496406793594, Validation Loss: 0.19831654642309463, Validation Accuracy: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:35<00:00,  1.60it/s]\n",
      "100%|██████████| 63/63 [00:38<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.19371293687820434, Validation Loss: 0.19579832823503585, Validation Accuracy: 0.93102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:39<00:00,  1.57it/s]\n",
      "100%|██████████| 63/63 [00:38<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.19058597487211226, Validation Loss: 0.18565417329470316, Validation Accuracy: 0.92944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:41<00:00,  1.55it/s]\n",
      "100%|██████████| 63/63 [00:39<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.1853734034001827, Validation Loss: 0.18514088245611343, Validation Accuracy: 0.93048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:58<00:00,  1.40it/s]\n",
      "100%|██████████| 63/63 [00:39<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.18152281159162523, Validation Loss: 0.19107283438955033, Validation Accuracy: 0.92926\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "\n",
    "for epoch in range(5):\n",
    "    model_vgg16.train()  # Training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model_vgg16(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model_vgg16.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model_vgg16(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = outputs > 0.5  # Using 0.5 as threshold\n",
    "            correct_preds += (predicted == labels.byte()).sum().item()\n",
    "            total_preds += labels.size(0) * labels.size(1)\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader)\n",
    "    epoch_val_loss = val_running_loss / len(val_loader)\n",
    "    epoch_val_accuracy = correct_preds / total_preds\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {epoch_train_loss}, Validation Loss: {epoch_val_loss}, Validation Accuracy: {epoch_val_accuracy}\")\n",
    "\n",
    "    # Recording the metrics for this epoch\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['val_accuracy'].append(epoch_val_accuracy)\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy and loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting sample predicted outputs vs true labels from validation set\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    tensor = tensor.to('cpu').detach().numpy().transpose((1, 2, 0))\n",
    "    tensor = np.clip(tensor, 0, 1)\n",
    "    return tensor\n",
    "\n",
    "def show_images_with_predictions(dataloader, model, device, categories, num_images=6):\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    plt.figure(figsize=(30, 30))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "\n",
    "                predicted_labels = (outputs[j] > 0.5).int()\n",
    "                pred_labels_text = [categories[idx][\"name\"] for idx, label in enumerate(predicted_labels) if label == 1]\n",
    "                true_labels_text = [categories[idx][\"name\"] for idx, label in enumerate(labels[j]) if label == 1]\n",
    "\n",
    "                ax.set_title(f\"True: {true_labels_text}\\nPred: {pred_labels_text}\")\n",
    "                plt.imshow(tensor_to_image(inputs.cpu().data[j]))\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train()\n",
    "                    return\n",
    "        model.train()\n",
    "\n",
    "show_images_with_predictions(val_loader, model_vgg16, device, categories)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
